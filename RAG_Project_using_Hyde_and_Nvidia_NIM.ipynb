{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0aZ/Cf85zTd79of8dshiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MFaiqKhan/RAG-Hyde-and-NVIDIA-NIM/blob/main/RAG_Project_using_Hyde_and_Nvidia_NIM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project: RAG USING Hyde and NVIDIA NIM(Nvidia Inference Models)"
      ],
      "metadata": {
        "id": "KrBxVbeEGSvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentation at the Bottom"
      ],
      "metadata": {
        "id": "rHh6FAG4GOAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "7HkRHXISWEko",
        "outputId": "9706b582-eff1-4f7a-e6ad-a7a947e77bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_nvidia_ai_endpoints\n",
            "  Downloading langchain_nvidia_ai_endpoints-0.1.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from langchain_nvidia_ai_endpoints) (3.9.5)\n",
            "Collecting langchain-core<0.3,>=0.1.27 (from langchain_nvidia_ai_endpoints)\n",
            "  Downloading langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow<11.0.0,>=10.0.0 (from langchain_nvidia_ai_endpoints)\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.65 (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading langsmith-0.1.67-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<24.0,>=23.2 (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.7.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (8.3.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (4.12.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2024.2.2)\n",
            "Installing collected packages: pillow, packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain_nvidia_ai_endpoints\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-core-0.2.3 langchain_nvidia_ai_endpoints-0.1.0 langsmith-0.1.67 orjson-3.10.3 packaging-23.2 pillow-10.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "7b45c101ed1142e19cfc41795a5f473e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain_nvidia_ai_endpoints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CPJ_cLvXZHDv",
        "outputId": "aee2a45c-8431-407e-b880-61ea2d162fb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain-community)\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.6 langchain-0.2.1 langchain-community-0.2.1 langchain-text-splitters-0.2.0 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MqPp0OaIbASw",
        "outputId": "88dc1b8d-bb8c-431d-8e6f-b6bc310f8ef1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/v0.1/docs/integrations/chat/nvidia_ai_endpoints/\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA"
      ],
      "metadata": {
        "id": "5bSLrvOxc2FR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader"
      ],
      "metadata": {
        "id": "5MlJngVgvvda"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct\")"
      ],
      "metadata": {
        "id": "fHqaJDHyv8Fv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "r00yx_oOxD7M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "k5Kj-MDvxQoJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"NVIDIA_API_KEY\"] = userdata.get('NVIDIA_API_KEY')"
      ],
      "metadata": {
        "id": "1wmBxA3JxYCf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = NVIDIAEmbeddings()"
      ],
      "metadata": {
        "id": "lkkwPotFyle8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "KWW9t7gay4KP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
      ],
      "metadata": {
        "id": "FoooKlorzCO0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "dpAsr4RFzKtK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JnkCdI9xBpos",
        "outputId": "2f2565a9-c515-4c6c-acbd-37cc62316638"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='mistralai / mistral-7b-instruct', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='Jump to ContentGuidesAPI ReferenceChangelogDiscussionsDocsExploreDocsExploreGuidesAPI ReferenceChangelogDiscussionsLoading…SearchJUMP TOLarge Language modelsaisingapore / sea-lion-7b-instructCreate a chat completionpostdatabricks / dbrx-instructCreate a chat completionpostgoogle / gemma-7bCreate a chat completionpostgoogle / gemma-2bCreate a chat completionpostgoogle / codegemma-1.1-7bCreates a model response for the given chat conversation.postgoogle / codegemma-7bCreate a chat', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='/ codegemma-7bCreate a chat completionpostgoogle / recurrentgemma-2bCreate a chat completionpostibm / granite-34b-code-instructCreates a model response for the given chat conversation.postibm / granite-8b-code-instructCreates a model response for the given chat conversation.postmediatek / breeze-7b-instructCreates a model response for the given chat conversation.postmeta / codellama-70bCreate a chat completionpostmeta / llama2-70bCreate a chat completionpostmeta / llama3-8bCreates a chat', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='chat completionpostmeta / llama3-8bCreates a chat completionpostmeta / llama3-70bCreates a chat completionpostmicrosoft / phi-3-medium-4k-instructCreates a chat completionpostmicrosoft / phi-3-mini-128k-instructCreates a chat completionpostmicrosoft / phi-3-mini-4k-instructCreates a chat completionpostmicrosoft / phi-3-small-128k-instructCreates a chat completionpostmicrosoft / phi-3-small-8k-instructCreate a chat completionpostmistralai / mistral-7b-instructCreate a chat', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='/ mistral-7b-instructCreate a chat completionpostmistralai / mixtral-8x7b-instructCreate a chat completionpostmistralai / mixtral-8x22b-instructCreate a chat completionpostmistralai / mistral-largeCreate a chat completionpostseallms / seallm-7b-v2.5Create a chat completionpostsnowflake / arcticCreate a chat completionpostRetrievalbaai / bge-m3Creates an embedding vector from the input text.postGets the result of an earlier function invocation request that returned a status of 202.getnvidia /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='request that returned a status of 202.getnvidia / embed-qa-4Create embedding vectorpostnvidia / rerank-qa-mistral-4bCreate rankingpostsnowflake / arctic-embed-lCreates an embedding vector from the input text.postGets the result of an earlier function invocation request that returned a status of 202.getVisual Modelsnvidia / retail-object-detectionRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getnvidia /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='request that returned a status of 202.getnvidia / visual-changenetRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getstability-ai / sdxl-turboRequest generationpoststability-ai / stable-diffusion-xlRequest generationpoststability-ai / stable-video-diffusionRequest generationpostmultimodAladept / fuyu-8bRequest response from the modelpostStatus pollinggetgoogle / deplotRequest response from the modelpostStatus', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='/ deplotRequest response from the modelpostStatus pollinggetgoogle / paligemmaRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getmicrosoft / kosmos-2Request response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getmicrosoft / phi-3-vision-128k-instructRequest response from the modelpostGets the result of an earlier function invocation request that returned a', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='function invocation request that returned a status of 202.getnvidia / neva-22bRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getHealthcareipd / rfdiffusionRun RFdiffusion Protein Generationpostmit / diffdockPredict molecular dockingpostmeta / esmfoldPredict protein structure (alignment-free)postnvidia / molmimPerform molecule generationpostnvidia / deepvariantRun Parabricks Universal Variant Callingpostnvidia /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='Parabricks Universal Variant Callingpostnvidia / fq2bamRun Parabricks fq2bam to align sequence readspostnvidia / vista3dRun Inferencepostroute optimizationnvidia / cuOptSubmit to solverpostStatus pollinggetclimate simulationnvidia / ai-weather-forecastingSubmit an inference configurationpostJUMP TOLarge Language modelsaisingapore / sea-lion-7b-instructCreate a chat completionpostdatabricks / dbrx-instructCreate a chat completionpostgoogle / gemma-7bCreate a chat completionpostgoogle /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='/ gemma-7bCreate a chat completionpostgoogle / gemma-2bCreate a chat completionpostgoogle / codegemma-1.1-7bCreates a model response for the given chat conversation.postgoogle / codegemma-7bCreate a chat completionpostgoogle / recurrentgemma-2bCreate a chat completionpostibm / granite-34b-code-instructCreates a model response for the given chat conversation.postibm / granite-8b-code-instructCreates a model response for the given chat conversation.postmediatek / breeze-7b-instructCreates a model', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='/ breeze-7b-instructCreates a model response for the given chat conversation.postmeta / codellama-70bCreate a chat completionpostmeta / llama2-70bCreate a chat completionpostmeta / llama3-8bCreates a chat completionpostmeta / llama3-70bCreates a chat completionpostmicrosoft / phi-3-medium-4k-instructCreates a chat completionpostmicrosoft / phi-3-mini-128k-instructCreates a chat completionpostmicrosoft / phi-3-mini-4k-instructCreates a chat completionpostmicrosoft /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='a chat completionpostmicrosoft / phi-3-small-128k-instructCreates a chat completionpostmicrosoft / phi-3-small-8k-instructCreate a chat completionpostmistralai / mistral-7b-instructCreate a chat completionpostmistralai / mixtral-8x7b-instructCreate a chat completionpostmistralai / mixtral-8x22b-instructCreate a chat completionpostmistralai / mistral-largeCreate a chat completionpostseallms / seallm-7b-v2.5Create a chat completionpostsnowflake / arcticCreate a chat completionpostRetrievalbaai /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='arcticCreate a chat completionpostRetrievalbaai / bge-m3Creates an embedding vector from the input text.postGets the result of an earlier function invocation request that returned a status of 202.getnvidia / embed-qa-4Create embedding vectorpostnvidia / rerank-qa-mistral-4bCreate rankingpostsnowflake / arctic-embed-lCreates an embedding vector from the input text.postGets the result of an earlier function invocation request that returned a status of 202.getVisual Modelsnvidia /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='returned a status of 202.getVisual Modelsnvidia / retail-object-detectionRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getnvidia / visual-changenetRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getstability-ai / sdxl-turboRequest generationpoststability-ai / stable-diffusion-xlRequest generationpoststability-ai / stable-video-diffusionRequest', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='/ stable-video-diffusionRequest generationpostmultimodAladept / fuyu-8bRequest response from the modelpostStatus pollinggetgoogle / deplotRequest response from the modelpostStatus pollinggetgoogle / paligemmaRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getmicrosoft / kosmos-2Request response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getmicrosoft /', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='that returned a status of 202.getmicrosoft / phi-3-vision-128k-instructRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getnvidia / neva-22bRequest response from the modelpostGets the result of an earlier function invocation request that returned a status of 202.getHealthcareipd / rfdiffusionRun RFdiffusion Protein Generationpostmit / diffdockPredict molecular dockingpostmeta / esmfoldPredict protein structure', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='/ esmfoldPredict protein structure (alignment-free)postnvidia / molmimPerform molecule generationpostnvidia / deepvariantRun Parabricks Universal Variant Callingpostnvidia / fq2bamRun Parabricks fq2bam to align sequence readspostnvidia / vista3dRun Inferencepostroute optimizationnvidia / cuOptSubmit to solverpostStatus pollinggetclimate simulationnvidia / ai-weather-forecastingSubmit an inference configurationpostmistralai / mistral-7b-instructModel Overview', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='Description:\\nMistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.\\nIt is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation datasets.\\nThird-Party Community Consideration:', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content=\"Third-Party Community Consideration:\\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see Mistral's 7B Instruct Hugging Face Model Card.\\nTerms of use\\nBy using this software or model, you are agreeing to the terms and conditions of the license, acceptable use policy and Mistral's privacy policy. Mistral-7B is released under the Apache 2.0 license\\nReferences(s):\", metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='References(s):\\nMistral 7B Instruct Model Card on Hugging Face \\nMistral 7B paper \\nMistral 7B blogpost \\nModel Architecture:\\nArchitecture Type: Transformer \\nNetwork Architecture: Mistral-7B \\nModel Version: 0.1 \\nInput:\\nInput Format: Text \\nInput Parameters: Max Tokens, Temperature, Top P \\nOutput:\\nOutput Format: Text \\nOutput Parameters: None \\nSoftware Integration:\\nSupported Hardware Platform(s): Hopper, Ampere, Turing \\nSupported Operating System(s): Linux \\nInference:\\nEngine: Triton', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='Inference:\\nEngine: Triton \\nTest Hardware: Other \\nEthical Considerations:', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards [Insert Link to Model Card++ here].  Please report security vulnerabilities or NVIDIA AI Concerns here.Table of Contents', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'}),\n",
              " Document(page_content='Model Overview\\n\\nDescription:\\nThird-Party Community Consideration:\\nTerms of use\\nReferences(s):\\nModel Architecture:\\nInput:\\nOutput:\\nSoftware Integration:\\n\\n\\n\\nInference:\\n\\nEthical Considerations:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTerms of Use |\\nPrivacy Policy |\\nManage My Privacy |\\nContact\\n\\nCopyright© 2024 NVIDIA Corporation', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'})]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJIxAAXHzT-G",
        "outputId": "72a6327e-f01f-4b4d-d0c5-356f944b19d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='mistralai / mistral-7b-instruct', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKC_wrUVB2s_",
        "outputId": "005f6deb-80eb-4edf-84cf-ae1e734aa728"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Jump to ContentGuidesAPI ReferenceChangelogDiscussionsDocsExploreDocsExploreGuidesAPI ReferenceChangelogDiscussionsLoading…SearchJUMP TOLarge Language modelsaisingapore / sea-lion-7b-instructCreate a chat completionpostdatabricks / dbrx-instructCreate a chat completionpostgoogle / gemma-7bCreate a chat completionpostgoogle / gemma-2bCreate a chat completionpostgoogle / codegemma-1.1-7bCreates a model response for the given chat conversation.postgoogle / codegemma-7bCreate a chat', metadata={'source': 'https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct', 'title': 'mistralai / mistral-7b-instruct', 'description': 'Model Overview Description: Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats.&lt;br/&gt;It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation data...', 'language': 'en'})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' creating a vectorstore '''\n",
        "\n",
        "vector = FAISS.from_documents(documents, embeddings)\n"
      ],
      "metadata": {
        "id": "fYqNbxRVzZsd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector.as_retriever()"
      ],
      "metadata": {
        "id": "tM3qfMxnzpa8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Hypothetical Embeddings\n",
        "# Using Langchain expression Language, Modern way of writing langchain\n",
        "\n",
        "# https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/\n",
        "\n",
        "# output parsers allow us to output in a good format\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "P86F7jMBzu0B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatNVIDIA(model=\"mistral_7b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqyfsEIS0X5l",
        "outputId": "2dc5fae7-7ec4-40ed-bc5a-eabdd663f82c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model mistral_7b is deprecated. Using mistralai/mistral-7b-instruct-v0.2 instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Go3X5KR0vXA",
        "outputId": "28b41b46-cb74-483c-d6db-653041625d8d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model='mistralai/mistral-7b-instruct-v0.2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothetical Embeddings or short for hyde.\n",
        "\n",
        "# After firing a Query to a RAG Pipeline it generates me a Hypothetical document and that doc is embedded\n",
        "# seprately and that embedding is utilized to look up real docs that are similar to hypothetical doc.\n",
        "# It enhances the retrieval"
      ],
      "metadata": {
        "id": "_U-lHVZd1EnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_template = \"\"\"\n",
        "Try Generating a Brief Level Paragraph for the Below Question even if Accurate\n",
        "or Perfect or Full Answer you don't possess:\n",
        "\n",
        "{question}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HP2PIqup1i6L"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)"
      ],
      "metadata": {
        "id": "tUolLhMb2woe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0eTQi-N22y8",
        "outputId": "b76c4db3-f87c-4699-9eb0-dd1645dd1b24"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template=\"\\nTry Generating a Brief Level Paragraph for the Below Question even if Accurate \\nor Perfect or Full Answer you don't possess:\\n\\n{question}\\n\\n\"))])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_query_transformer = hyde_prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "WWJ0Rf1x3PzH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_query_transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHUUFHyH3ctD",
        "outputId": "6073d55d-1852-46f3-eca0-20fc6e005b7e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template=\"\\nTry Generating a Brief Level Paragraph for the Below Question even if Accurate \\nor Perfect or Full Answer you don't possess:\\n\\n{question}\\n\\n\"))])\n",
              "| ChatNVIDIA(model='mistralai/mistral-7b-instruct-v0.2')\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import chain"
      ],
      "metadata": {
        "id": "yYd6LEke3KlF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "def hyde_ret(question):\n",
        "  hypothetical_doc = hyde_query_transformer.invoke({\"question\": question})\n",
        "  return retriever.invoke(hypothetical_doc)"
      ],
      "metadata": {
        "id": "zrfvk_s931j1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Step by Step with number and next line.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6hHbwnMg366I"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "eG1GhlQp4dIi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_chain = prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "wpUBSTnA4hg4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "def answer(question):\n",
        "  docs = hyde_ret.invoke(question)\n",
        "  for s in answer_chain.stream({\"question\": question, \"context\": docs}):\n",
        "    yield s"
      ],
      "metadata": {
        "id": "mzVC1Ilg4niH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in answer.stream(\"Explain How Mistral Works in Nvidia NIM\"):\n",
        "  print(s, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WAC78iD44E7",
        "outputId": "c039dac7-fdc6-409a-dcad-f4bdbba8fa71"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mistral is a language model developed by a third-party called Mistral, which has been made available through Nvidia's NIM (Nvidia Inferences Model) platform. The Mistral-7B-Instruct model is an instruct version of the Mistral-7B generative text model, fine-tuned using various publicly available conversation datasets. It can follow instructions, complete requests, and generate creative text formats.\n",
            "\n",
            "Mistral works on the Nvidia NIM platform by making requests to the model using specific APIs and functions provided by Nvidia. The input to the model is text, and the output is also text. The user can set input parameters like Max Tokens, Temperature, and Top P.\n",
            "\n",
            "The software is supported on various hardware platforms like Hopper, Ampere, and Turing, and operating systems like Linux. The inference engine used is Triton. Examples of functions that can be used with Mistral include creating a chat completion and retrieving the result of an earlier function invocation. The model is released under the Apache 2.0 license."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcEZmHYt438z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code provided sets up an information retrieval pipeline using NVIDIA's Mistral AI model, Langchain's utilities, and FAISS for vector storage. Here's a breakdown of the code along with explanations:\n",
        "\n",
        "### Setup and Installations\n",
        "\n",
        "```python\n",
        "!pip install langchain_nvidia_ai_endpoints\n",
        "!pip install langchain-community langchain-text-splitters\n",
        "!pip install faiss-cpu\n",
        "```\n",
        "\n",
        "These commands install necessary packages:\n",
        "- `langchain_nvidia_ai_endpoints`: Provides access to NVIDIA AI endpoints.\n",
        "- `langchain-community` and `langchain-text-splitters`: Community and text splitter utilities for Langchain.\n",
        "- `faiss-cpu`: FAISS library for efficient similarity search.\n",
        "\n",
        "### Document Loading\n",
        "\n",
        "```python\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.api.nvidia.com/nim/reference/mistralai-mistral-7b-instruct\")\n",
        "docs = loader.load()\n",
        "```\n",
        "\n",
        "This section loads documents from a specified URL using the `WebBaseLoader`.\n",
        "\n",
        "### Environment Setup\n",
        "\n",
        "```python\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"NVIDIA_API_KEY\"] = userdata.get('NVIDIA_API_KEY')\n",
        "```\n",
        "\n",
        "It sets the NVIDIA API key from the Colab's user data.\n",
        "\n",
        "### Embedding and Text Splitting\n",
        "\n",
        "```python\n",
        "embeddings = NVIDIAEmbeddings()\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "```\n",
        "\n",
        "This code initializes the NVIDIA embeddings and splits the loaded documents into chunks of 500 characters with a 50-character overlap.\n",
        "\n",
        "### Creating a Vector Store\n",
        "\n",
        "```python\n",
        "vector = FAISS.from_documents(documents, embeddings)\n",
        "retriever = vector.as_retriever()\n",
        "```\n",
        "\n",
        "The document chunks are then stored in a FAISS vector store, and a retriever is created to fetch relevant documents based on embeddings.\n",
        "\n",
        "### Model Setup and Prompts\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "model = ChatNVIDIA(model=\"mistral_7b\")\n",
        "```\n",
        "\n",
        "The NVIDIA Mistral model is initialized. Prompts and output parsers are also set up.\n",
        "\n",
        "### Hypothetical Embeddings (HyDE) Setup\n",
        "\n",
        "```python\n",
        "hyde_template = \"\"\"\n",
        "Try Generating a Brief Level Paragraph for the Below Question even if Accurate\n",
        "or Perfect or Full Answer you don't possess:\n",
        "\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)\n",
        "hyde_query_transformer = hyde_prompt | model | StrOutputParser()\n",
        "```\n",
        "\n",
        "A hypothetical embeddings prompt is created to generate a brief paragraph for a given question.\n",
        "\n",
        "### Retrieval and Answer Chain\n",
        "\n",
        "```python\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "@chain\n",
        "def hyde_ret(question):\n",
        "  hypothetical_doc = hyde_query_transformer.invoke({\"question\": question})\n",
        "  return retriever.invoke(hypothetical_doc)\n",
        "\n",
        "template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Step by Step with number and next line.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "answer_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "@chain\n",
        "def answer(question):\n",
        "  docs = hyde_ret.invoke(question)\n",
        "  for s in answer_chain.stream({\"question\": question, \"context\": docs}):\n",
        "    yield s\n",
        "```\n",
        "\n",
        "Two chained functions are defined:\n",
        "- `hyde_ret`: Generates a hypothetical document for a given question and retrieves relevant documents using the retriever.\n",
        "- `answer`: Uses the retrieved documents to generate a final answer.\n",
        "\n",
        "### Streaming the Answer\n",
        "\n",
        "```python\n",
        "for s in answer.stream(\"Explain How Mistral Works in Nvidia NIM\"):\n",
        "  print(s, end=\"\")\n",
        "```\n",
        "\n",
        "This code initiates the answer retrieval process for the question \"Explain How Mistral Works in Nvidia NIM\" and prints the results.\n",
        "\n",
        "### Summary\n",
        "\n",
        "This script demonstrates how to set up a Retrieval-Augmented Generation (RAG) pipeline using NVIDIA's Mistral model, Langchain's utilities, and FAISS. The pipeline consists of loading and splitting documents, creating a vector store, setting up hypothetical embeddings, retrieving relevant documents, and generating answers based on the retrieved context."
      ],
      "metadata": {
        "id": "ygiQ5FPyGLZo"
      }
    }
  ]
}